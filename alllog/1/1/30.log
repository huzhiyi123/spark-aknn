gist efConstruction 

for i in efConstructionlist: cmp 20
type(tmp),tmp[0:100],tmp.shape <class 'numpy.ndarray'> [[13]
 [13]
 [13]
 [28]
 [21]] (1000000, 1) (1000000, 3)
centroids1 (4, 128)
centroids2 (40, 128)
print(curdf[0]),curdf.shape (50000,) <class 'pandas.core.series.Series'>
start query
result.count() 10000
end query
l 10000 k 10
recall = cnt/float(l*k) 69706   l 10000 k 10
recall: 0.69706
totalconstructtime 248546.03457450867 kmeanspartitiontime 12047.041893005371 localindexconstructtime 234713.71698379517 globalindexconstructtime 1785.2756977081299
totalsearchtime 13719.430685043335 localsearchtime 13586.925506591797 globalsearchtime 132.5051784515381
hello world testdoublekmeansHnsw

for i in efConstructionlist: cmp 50
type(tmp),tmp[0:100],tmp.shape <class 'numpy.ndarray'> [[13]
 [13]
 [13]
 [28]
 [21]] (1000000, 1) (1000000, 3)
centroids1 (4, 128)
centroids2 (40, 128)
print(curdf[0]),curdf.shape (50000,) <class 'pandas.core.series.Series'>
start query
result.count() 10000
end query
l 10000 k 10
recall = cnt/float(l*k) 75515   l 10000 k 10
recall: 0.75515
totalconstructtime 247782.68194198608 kmeanspartitiontime 10395.265340805054 localindexconstructtime 235603.43265533447 globalindexconstructtime 1783.9839458465576
totalsearchtime 17284.46078300476 localsearchtime 17163.413524627686 globalsearchtime 121.0472583770752
hello world testdoublekmeansHnsw

for i in efConstructionlist: cmp 100
type(tmp),tmp[0:100],tmp.shape <class 'numpy.ndarray'> [[13]
 [13]
 [13]
 [28]
 [21]] (1000000, 1) (1000000, 3)
centroids1 (4, 128)
centroids2 (40, 128)
print(curdf[0]),curdf.shape (50000,) <class 'pandas.core.series.Series'>
start query
result.count() 10000
end query
l 10000 k 10
recall = cnt/float(l*k) 77898   l 10000 k 10
recall: 0.77898
totalconstructtime 501966.33195877075 kmeanspartitiontime 11873.35467338562 localindexconstructtime 488234.9982261658 globalindexconstructtime 1857.9790592193604
totalsearchtime 27252.995014190674 localsearchtime 27124.61018562317 globalsearchtime 128.38482856750488
hello world testdoublekmeansHnsw

for i in efConstructionlist: cmp 150
type(tmp),tmp[0:100],tmp.shape <class 'numpy.ndarray'> [[13]
 [13]
 [13]
 [28]
 [21]] (1000000, 1) (1000000, 3)
centroids1 (4, 128)
centroids2 (40, 128)
print(curdf[0]),curdf.shape (50000,) <class 'pandas.core.series.Series'>
start query
result.count() 10000
end query
l 10000 k 10
recall = cnt/float(l*k) 78060   l 10000 k 10
recall: 0.7806
totalconstructtime 732711.2102508545 kmeanspartitiontime 10930.209636688232 localindexconstructtime 719941.305398941 globalindexconstructtime 1839.6952152252197
totalsearchtime 37191.79940223694 localsearchtime 37061.267614364624 globalsearchtime 130.53178787231445
hello world testdoublekmeansHnsw

for i in efConstructionlist: cmp 200
type(tmp),tmp[0:100],tmp.shape <class 'numpy.ndarray'> [[13]
 [13]
 [13]
 [28]
 [21]] (1000000, 1) (1000000, 3)
centroids1 (4, 128)
centroids2 (40, 128)
print(curdf[0]),curdf.shape (50000,) <class 'pandas.core.series.Series'>
start query
result.count() 10000
end query
l 10000 k 10
recall = cnt/float(l*k) 78338   l 10000 k 10
recall: 0.78338
totalconstructtime 860915.4937267303 kmeanspartitiontime 10633.985757827759 localindexconstructtime 848769.7460651398 globalindexconstructtime 1511.7619037628174
totalsearchtime 38097.14317321777 localsearchtime 38003.36527824402 globalsearchtime 93.77789497375488
hello world testdoublekmeansHnsw

for i in efConstructionlist: cmp 300
type(tmp),tmp[0:100],tmp.shape <class 'numpy.ndarray'> [[13]
 [13]
 [13]
 [28]
 [21]] (1000000, 1) (1000000, 3)
centroids1 (4, 128)
centroids2 (40, 128)
Traceback (most recent call last):
  File "/aknn/test/testdoublekmeans.py", line 447, in <module>
    testdoublekmeansHnswV2()
  File "/aknn/test/testdoublekmeans.py", line 373, in testdoublekmeansHnswV2
    model=hnsw.fit(words_df)
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/ml/base.py", line 132, in fit
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/ml/wrapper.py", line 295, in _fit
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/ml/wrapper.py", line 292, in _fit_java
  File "/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    print("Adding first batch of %d elements" % (len(data)))
  File "/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o1224.fit.
: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: FAILED
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:933)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply$mcV$sp(Dataset.scala:2735)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2735)
	at org.apache.spark.sql.Dataset$$anonfun$foreachPartition$1.apply(Dataset.scala:2735)
	at org.apache.spark.sql.Dataset$$anonfun$withNewRDDExecutionId$1.apply(Dataset.scala:3349)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withNewRDDExecutionId(Dataset.scala:3345)
	at org.apache.spark.sql.Dataset.foreachPartition(Dataset.scala:2734)
	at com.github.jelmerk.spark.knn.KnnAlgorithm.typedFit(KnnAlgorithm.scala:854)
	at com.github.jelmerk.spark.knn.KnnAlgorithm.fit(KnnAlgorithm.scala:733)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

