gist efConstruction 

gist bruteForce klist = [5,10,20,30,40,50]   efConstruction = 150 usesift = False 10
predict.count() 10000
root
 |-- id: integer (nullable = false)
 |-- approximate: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- neighbor: integer (nullable = false)
 |    |    |-- distance: double (nullable = false)

timeUsed:  1050297.7550029755 fit time 17205.20329475403
hello world bruteForce

gist bruteForce klist = [5,10,20,30,40,50]   efConstruction = 150 usesift = False 20
predict.count() 10000
root
 |-- id: integer (nullable = false)
 |-- approximate: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- neighbor: integer (nullable = false)
 |    |    |-- distance: double (nullable = false)

timeUsed:  902001.2121200562 fit time 10012.908458709717
hello world bruteForce

gist bruteForce klist = [5,10,20,30,40,50]   efConstruction = 150 usesift = False 30
predict.count() 10000
root
 |-- id: integer (nullable = false)
 |-- approximate: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- neighbor: integer (nullable = false)
 |    |    |-- distance: double (nullable = false)

timeUsed:  909549.5934486389 fit time 10022.817134857178
hello world bruteForce

gist bruteForce klist = [5,10,20,30,40,50]   efConstruction = 150 usesift = False 40
predict.count() 10000
root
 |-- id: integer (nullable = false)
 |-- approximate: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- neighbor: integer (nullable = false)
 |    |    |-- distance: double (nullable = false)

timeUsed:  951776.0305404663 fit time 9767.499685287476
hello world bruteForce

gist bruteForce klist = [5,10,20,30,40,50]   efConstruction = 150 usesift = False 50
Traceback (most recent call last):
  File "/aknn/test/testdoublekmeans.py", line 447, in <module>
    bruteForce()
  File "/aknn/test/testdoublekmeans.py", line 267, in bruteForce
    print("predict.count()",predict.count())
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 522, in count
  File "/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    print("Adding first batch of %d elements" % (len(data)))
  File "/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o950.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: FAILED
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)
	at org.apache.spark.rdd.RDD.collect(RDD.scala:944)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)
	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2830)
	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2829)
	at org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:2829)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)

maxelement = 100000000
k=10
partitionnum=8
topkPartitionNum=3

sc = 1
m = int(50)
distanceFunction='cosine'
kmeanstrainrate=0.05

efConstruction=35
ef = int(4*22)
usesift=True

kmeanspath="/aknn/kmeans/"

gistlist=["gistpartition.csv","gistcentroids2.csv","gistcentroids1.csv"]  
siftlist=["siftpartition.csv","siftcentroids1.csv","siftcentroids2.csv"]

"""
datapath="/my/siftsmall/"
traindatapath=datapath+"siftsmall_base.fvecs"
querydatapath=datapath+"siftsmall_query.fvecs"
querygroundtruthpath=datapath+"siftsmall_groundtruth.ivecs"
"""
datapath="/data/"
traindatapath=datapath+"sift_base.fvecs"
querydatapath=datapath+"sift_query.fvecs"
querygroundtruthpath=datapath+"sift_groundtruth.ivecs"


gistpath="/data/mnist.hdf5"
# ef=10, efConstruction=200


def initparams():
    global maxelement,k,partitionnum,topkPartitionNum,ef,m,distanceFunction,kmeanstrainraten,efConstruction,usesift
    maxelement = 100000000
    k=10
    partitionnum=8
    topkPartitionNum=5
    
    sc = 1
    m = int(50)
    distanceFunction='cosine'
    kmeanstrainrate=0.05
    efConstruction=85
    ef = efConstruction
    usesift=True