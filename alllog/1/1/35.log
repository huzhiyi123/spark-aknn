gist efConstruction 

for i in efConstructionlist: cmp 15
type(tmp),tmp[0:100],tmp.shape <class 'numpy.ndarray'> [[20]
 [13]
 [13]
 [13]
 [28]] (1000000, 1) (1000000, 3)
centroids1 (5, 128)
centroids2 (41, 128)
print(curdf[0]),curdf.shape (50000,) <class 'pandas.core.series.Series'>
start query
result.count() 10000
end query
l 10000 k 10
recall = cnt/float(l*k) 76989   l 10000 k 10
recall: 0.76989
totalconstructtime 248292.2384738922 kmeanspartitiontime 12147.863864898682 localindexconstructtime 234618.36504936218 globalindexconstructtime 1526.0095596313477
totalsearchtime 13743.048906326294 localsearchtime 13614.34030532837 globalsearchtime 128.7086009979248
hello world testdoublekmeansHnsw

for i in efConstructionlist: cmp 50
type(tmp),tmp[0:100],tmp.shape <class 'numpy.ndarray'> [[20]
 [13]
 [13]
 [13]
 [28]] (1000000, 1) (1000000, 3)
centroids1 (5, 128)
centroids2 (41, 128)
Traceback (most recent call last):
  File "/aknn/test/testdoublekmeans.py", line 441, in <module>
    testdoublekmeansHnswV2()
  File "/aknn/test/testdoublekmeans.py", line 366, in testdoublekmeansHnswV2
    model=hnsw.fit(words_df)
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/ml/base.py", line 132, in fit
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/ml/wrapper.py", line 295, in _fit
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/ml/wrapper.py", line 292, in _fit_java
  File "/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/usr/spark-2.4.1/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    print("Adding first batch of %d elements" % (len(data)))
  File "/usr/spark-2.4.1/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o296.fit.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 1.0 failed 4 times, most recent failure: Lost task 2.3 in stage 1.0 (TID 312, 172.18.0.3, executor 0): java.io.IOException: Mkdirs failed to create /tmp/HnswSimilarity_d36a1b94c5bc_1643204311154 (exists=false, cwd=file:/usr/spark-2.4.1/work/app-20220126133712-0012/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:458)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1103)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1083)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:972)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:960)
	at com.github.jelmerk.spark.knn.KnnAlgorithm$$anonfun$typedFit$1.apply(KnnAlgorithm.scala:870)
	at com.github.jelmerk.spark.knn.KnnAlgorithm$$anonfun$typedFit$1.apply(KnnAlgorithm.scala:854)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$28.apply(RDD.scala:935)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:121)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:403)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:409)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)
	at scala.collection.mutable.Resmaxelement = 100000000
k=10
partitionnum=8
topkPartitionNum=3

sc = 1
m = int(50)
distanceFunction='cosine'
kmeanstrainrate=0.05

efConstruction=35
ef = int(4*22)
usesift=True

kmeanspath="/aknn/kmeans/"

gistlist=["gistpartition.csv","gistcentroids2.csv","gistcentroids1.csv"]  
siftlist=["siftpartition.csv","siftcentroids1.csv","siftcentroids2.csv"]

"""
datapath="/my/siftsmall/"
traindatapath=datapath+"siftsmall_base.fvecs"
querydatapath=datapath+"siftsmall_query.fvecs"
querygroundtruthpath=datapath+"siftsmall_groundtruth.ivecs"
"""
datapath="/data/"
traindatapath=datapath+"sift_base.fvecs"
querydatapath=datapath+"sift_query.fvecs"
querygroundtruthpath=datapath+"sift_groundtruth.ivecs"


gistpath="/data/mnist.hdf5"
# ef=10, efConstruction=200


def initparams():
    global maxelement,k,partitionnum,topkPartitionNum,ef,m,distanceFunction,kmeanstrainraten,efConstruction,usesift
    maxelement = 100000000
    k=10
    partitionnum=4
    topkPartitionNum=3
    sc = 1
    m = int(50)
    distanceFunction='cosine'
    kmeanstrainrate=0.05
    efConstruction=85
    ef = efConstruction
    usesift=Truerecall = cnt/float(l*k) 76989   l 10000 k 10
recall: 0.76989
